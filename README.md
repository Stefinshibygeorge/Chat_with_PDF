# ğŸ“„ Chat with PDF (Streamlit + LLM + AstraDB)

A conversational PDF assistant built using LangChain, LLaMA, AstraDB, and Streamlit. Upload any PDF and ask natural questions â€” it retrieves relevant content and answers intelligently, with memory and customizable styles.

---

## ğŸš€ Features

- ğŸ“ Upload any PDF
- ğŸ” Context-aware question answering
- ğŸ§  Conversational memory (adjustable)
- âœï¸ Custom answer styles: Short, Long, Explain
- ğŸ›ï¸ Word limit control
- ğŸ“œ Clean WhatsApp-style chat interface
- ğŸ›  Built with LangChain + HuggingFace + AstraDB + Streamlit

---

## ğŸ§° Tech Stack

- **Frontend**: Streamlit
- **LLM**: LLaMA 3 (via HuggingFace Inference Client)
- **Embeddings**: MiniLM (`all-MiniLM-L6-v2`)
- **Vector DB**: AstraDB (DataStax)
- **Memory**: Custom Q&A history injection
- **Styling**: External CSS + HTML templates

---

## ğŸ“¦ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/Stefinshibygeorge/chat-with-pdf.git
cd chat-with-pdf
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up environment variables

Create a `.env` file with:

```env
HF_TOKEN= <your_huggingface_api_key>
HF_PROVIDER= <fireworks-ai>
ASTRA_DB_ID= <your_astra_db_id>
ASTRA_DB_APPLICATION_TOKEN= <your_astra_token>
```

### 4. Run the app

```bash
streamlit run app.py
```

---

## ğŸ“ Project Structure

```
chat-with-pdf/
â”œâ”€â”€ app.py                  # Streamlit app
â”œâ”€â”€ inference_client.py     # LLM request logic
â”œâ”€â”€ utils.py                # PDF loading, vector DB setup
â”œâ”€â”€ main.py                 # test run, without UI
â”œâ”€â”€ html_templates.py       # Chat bubble HTML renderer
â”œâ”€â”€ style.css               # Chat UI styling
â”œâ”€â”€ config.py               # Global config variables
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                    # Environment secrets
```

---


## ğŸ›¡ License

MIT License
